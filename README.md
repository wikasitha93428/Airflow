## Airflow
------------------------------------------
Apache Airflow introduction Note & Testing 
-------------------------------------------
01: Prerequisites 
        1:Docker Desktop
        2:Astro CLI  
        https://www.astronomer.io/docs/astro/cli/install-cli?tab=linux#install-the-astro-cli 
            
1.1: curl -sSL install.astronomer.io | sudo bash -s
            ![image](https://github.com/user-attachments/assets/fcb83949-b594-4dd0-9aac-d5fe8e9ee559)
        
1.2: astro version  
            ![image](https://github.com/user-attachments/assets/6fac962b-0f42-4d52-a534-615e476a2033)


02: Basics of Airflow 
            Airflow is workflow orchestration tool, build, schedule pipelines 
            Airflow has multiple DAGs and each DAG has multiple tasks 
            Task = a unit of operation that we  need perform 
            ![image](https://github.com/user-attachments/assets/adf2a9e1-bccc-4f6b-8286-4abfa750e18e) 
            

03: Why do we need workflow orchestration? 
        The definition of data orchestration is the process of coordinating and automating the movement, 
        transformation and integration of data across different systems and processes to ensure efficient
        and reliable data workflows. 

04: Why we need Airflow?
        ![image](https://github.com/user-attachments/assets/e0a3c0c2-1aa2-434d-a7ec-f853544efb3c) 

05: dbt + snowflake + Airflow 
        ![image](https://github.com/user-attachments/assets/4843d502-d5f3-4502-b3af-f9ee4d2eb0ad)
        


        


        
